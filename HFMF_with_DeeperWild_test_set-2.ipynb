{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tvUbwdGQJsm"
      },
      "source": [
        "# Feature extraction for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TiadtflJwte"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLKUR93DQGmM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data transformations (ResNet-style transformations)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDatasetNew(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, limit=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        for label_folder in tqdm(['0_real', '1_fake'], desc=\"Loading dataset\"):\n",
        "            full_path = os.path.join(root_dir, label_folder)\n",
        "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
        "                if limit and idx >= limit:\n",
        "                    break  # Limit the number of files loaded\n",
        "                if file_name.endswith(('.jpg', '.png', '.jpeg','.JPG','.JPEG')):  # Ensure image files\n",
        "                    self.image_files.append(os.path.join(full_path, file_name))\n",
        "                    if 'real' in label_folder:\n",
        "                        self.labels.append(0)  # Label 0 for real images\n",
        "                    else:\n",
        "                        self.labels.append(1)  # Label 1 for fake images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]  # This is a string path\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_path  # Ensure that the path returned is a string (not a tensor)\n",
        "\n",
        "\n",
        "# Load ResNet model and capture features\n",
        "def load_saved_resnet_model(model_path):\n",
        "    model = torchvision.models.resnet50(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # Freeze all layers\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 2)  # Binary classification (real/fake)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))  # Load the saved model\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Hook functions to capture low, mid, and high-level features\n",
        "    model.layer1[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, low_level_features))\n",
        "    model.layer3[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, mid_level_features))\n",
        "    model.layer4[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, high_level_features))\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Hook functions to capture ResNet features\n",
        "low_level_features, mid_level_features, high_level_features = [], [], []\n",
        "\n",
        "def hook_fn(module, input, output, storage_list):\n",
        "    storage_list.append(output.clone().detach())\n",
        "\n",
        "# Define linear layers to convert ResNet features to 768 dimensions\n",
        "# Define linear layers to convert ResNet features to 768 dimensions\n",
        "low_to_768 = nn.Linear(256, 768).to(device)   # For low-level features\n",
        "mid_to_768 = nn.Linear(1024, 768).to(device)  # For mid-level features\n",
        "high_to_768 = nn.Linear(2048, 768).to(device) # For high-level features\n",
        "\n",
        "def extract_resnet_features(model, image):\n",
        "    low_level_features.clear()\n",
        "    mid_level_features.clear()\n",
        "    high_level_features.clear()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "        model(image)\n",
        "\n",
        "    # Pool ResNet features and map to 768 dimensions\n",
        "    low_pooled = F.adaptive_avg_pool2d(low_level_features[-1].to(device), (1, 1)).squeeze()\n",
        "    mid_pooled = F.adaptive_avg_pool2d(mid_level_features[-1].to(device), (1, 1)).squeeze()\n",
        "    high_pooled = F.adaptive_avg_pool2d(high_level_features[-1].to(device), (1, 1)).squeeze()\n",
        "\n",
        "    low_768 = low_to_768(low_pooled)   # Shape [1, 768]\n",
        "    mid_768 = mid_to_768(mid_pooled)   # Shape [1, 768]\n",
        "    high_768 = high_to_768(high_pooled) # Shape [1, 768]\n",
        "\n",
        "    return low_768, mid_768, high_768\n",
        "\n",
        "\n",
        "# Function to preprocess the image using ViT's transforms\n",
        "def pipeline_preprocessor():\n",
        "    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "    return vit_weights.transforms()\n",
        "\n",
        "# Function to extract ViT embeddings\n",
        "def get_vit_embedding(vit_model, image_path):\n",
        "    preprocessing = pipeline_preprocessor()  # Preprocessing from ViT\n",
        "    img = Image.open(image_path).convert(\"RGB\")  # Ensure we load image by path (string)\n",
        "    img = preprocessing(img).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = vit_model._process_input(img)\n",
        "        batch_class_token = vit_model.class_token.expand(img.shape[0], -1, -1)\n",
        "        feats = torch.cat([batch_class_token, feats], dim=1)\n",
        "        feats = vit_model.encoder(feats)\n",
        "        vit_hidden = feats[:, 0]  # CLS token\n",
        "    return vit_hidden\n",
        "\n",
        "# Load ViT model\n",
        "def load_vit_model(pretrained_weights_path):\n",
        "    vit_model = torchvision.models.vit_b_16(pretrained=False).to(device)\n",
        "    pretrained_vit_weights = torch.load(pretrained_weights_path, map_location=device)\n",
        "    vit_model.load_state_dict(pretrained_vit_weights, strict=False)\n",
        "    vit_model.eval()  # Set to evaluation mode\n",
        "    return vit_model\n",
        "\n",
        "# Add a sequence dimension (if missing) before applying attention\n",
        "def ensure_correct_shape(tensor):\n",
        "    if len(tensor.shape) == 2:  # If shape is [batch_size, embedding_dim]\n",
        "        tensor = tensor.unsqueeze(1)  # Add a sequence dimension: [batch_size, 1, embedding_dim]\n",
        "    elif len(tensor.shape) == 1:  # If shape is [embedding_dim]\n",
        "        tensor = tensor.unsqueeze(0).unsqueeze(1)  # Add batch and sequence dimensions: [1, 1, embedding_dim]\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# Scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    # Ensure Q, K, and V have the correct shapes\n",
        "    Q = ensure_correct_shape(Q)  # Should be [batch_size, 1, embedding_dim]\n",
        "    K = ensure_correct_shape(K)  # Should be [batch_size, 1, embedding_dim]\n",
        "    V = ensure_correct_shape(V)  # Should be [batch_size, 1, embedding_dim]\n",
        "\n",
        "#     print(f\"Q shape after unsqueeze: {Q.shape}, K shape after unsqueeze: {K.shape}, V shape after unsqueeze: {V.shape}\")  # Debugging\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32).to(Q.device))\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn_weights, V)\n",
        "    return output\n",
        "\n",
        "# Save features for each dataset (train/val/test)\n",
        "import csv\n",
        "\n",
        "# Save features for each dataset (train/val/test) as CSV\n",
        "def save_features_to_csv(model, vit_model, data_loader, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    with open(save_path, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write the CSV header\n",
        "        writer.writerow([\"image_name\", \"features\", \"label\"])\n",
        "\n",
        "        for images, img_paths in tqdm(data_loader, desc=\"Extracting features\"):\n",
        "            for i in range(len(images)):\n",
        "                image = images[i].to(device)  # Move image to the correct device\n",
        "                img_path = img_paths[i]  # Image path\n",
        "\n",
        "                # Ensure img_path is a string\n",
        "                if isinstance(img_path, torch.Tensor):\n",
        "                    img_path = img_path.item() if img_path.dim() == 0 else str(img_path)\n",
        "\n",
        "                # Extract ResNet features\n",
        "                try:\n",
        "                    low_768, mid_768, high_768 = extract_resnet_features(model, image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting ResNet features for {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract ViT features\n",
        "                try:\n",
        "                    vit_hidden = get_vit_embedding(vit_model, img_path)  # img_path should be a string\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting ViT features for {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Apply attention between ResNet and ViT features\n",
        "                try:\n",
        "                    output_1 = scaled_dot_product_attention(vit_hidden, low_768, low_768)\n",
        "                    output_2 = scaled_dot_product_attention(output_1, mid_768, mid_768)\n",
        "                    final_output = scaled_dot_product_attention(output_2, high_768, high_768)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error applying attention for {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert features to a flattened list\n",
        "                features = final_output.detach().cpu().numpy().flatten().tolist()\n",
        "\n",
        "\n",
        "                # Extract label from the image path\n",
        "                label = 0 if \"real\" in img_path else 1\n",
        "\n",
        "                # Write the row to the CSV\n",
        "                writer.writerow([os.path.basename(img_path), features, label])\n",
        "\n",
        "    print(f\"Features saved to {save_path}\")\n",
        "\n",
        "\n",
        "# Load models\n",
        "resnet_model = load_saved_resnet_model('/content/drive/MyDrive/Final_folder_code_thesis/Original_model/pretrained_resnet_state_dict.pth')\n",
        "vit_model = load_vit_model('/content/drive/MyDrive/Final_folder_code_thesis/Original_model/pretrained_vit_state_dict.pth')\n",
        "\n",
        "#train_dir = \"/content/WildRF/train\"\n",
        "#val_dir = \"/content/WildRF/val\"\n",
        "test_dir=\"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test\"\n",
        "\n",
        "#train_dataset = CustomDatasetNew(root_dir=train_dir, transform=data_transforms)\n",
        "#val_dataset = CustomDatasetNew(root_dir=val_dir, transform=data_transforms)\n",
        "#test_dataset = CustomDatasetNew(root_dir=test_dir, transform=data_transforms)\n",
        "for test_subdir in ['twitter', 'facebook', 'reddit']:\n",
        "    test_dataset = CustomDatasetNew(root_dir=os.path.join(test_dir, test_subdir), transform=data_transforms)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    print(f\"Processing Test Dataset: {test_subdir}\")\n",
        "\n",
        "    # Save features to a separate CSV file for each subdirectory\n",
        "    save_path = f\"/content/drive/MyDrive/Final_folder_code_thesis/New_test_set_test_features_{test_subdir}.csv\"\n",
        "    save_features_to_csv(resnet_model, vit_model, test_loader, save_path=save_path)\n",
        "\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "#test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "#print(\"Processing Train Dataset:\")\n",
        "#save_features_to_csv(resnet_model, vit_model, train_loader, save_path=\"features_WildRF/train_features.csv\")\n",
        "\n",
        "#print(\"Processing Validation Dataset:\")\n",
        "#save_features_to_csv(resnet_model, vit_model, val_loader, save_path=\"features_WildRF/val_features.csv\")\n",
        "\n",
        "#print(\"Processing Test Dataset:\")\n",
        "#save_features_to_csv(resnet_model, vit_model, test_loader, save_path=\"features_WildRF/test_features.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6GGX_0AQ_MY"
      },
      "source": [
        "#Importing libaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96s1dErRRFBJ"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzoKYDc0RXG1"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==5.26.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYx5WhcgRSOU"
      },
      "outputs": [],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "086DCId1RhVI"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe==0.10.21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPNw3G51XYsN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBW1b8y_RnZo"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe==0.10.21\n",
        "!pip install pretrainedmodels\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6ai6QWMczx"
      },
      "source": [
        "#Twitter test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntPTwb8QMmtt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srhukBv3Mnbn"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "test_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/New_test_set_test_features_twitter.csv\"\n",
        "test_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test/twitter\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/final_model1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "test_dataset = prepare_ensemble_data(test_csv, test_folder, transform, module1, module2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4OPjWKjMu6f",
        "outputId": "1a1fc599-ccef-4c44-ef47-56c51eba4c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy twitter: 55.39%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the accuracy function\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Assuming EnsembleModel is defined and loaded correctly\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "# Load the trained weights from the saved model\n",
        "ensemble_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/ensemble_model_weights.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ensemble_model.eval()\n",
        "\n",
        "# Now compute the accuracy on the test set\n",
        "test_accuracy = compute_accuracy(ensemble_model, test_loader, device)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy twitter: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rHtRMu_fH71"
      },
      "source": [
        "### ROC and AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxaFAPzGrRdW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "ensemble_model.eval()\n",
        "with torch.no_grad():\n",
        "    for module1_out, module2_out, labels in test_loader:\n",
        "        module1_out, module2_out = module1_out.to(device), module2_out.to(device)\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (fake)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# === ROC & AUC ===\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='magenta', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Deepfake Detection (Twitter Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=\"RdPu\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Deepfake Detection (Twitter Test Set)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# === Metrics ===\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, pos_label=1)\n",
        "recall = recall_score(all_labels, all_preds, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "print(f\"âœ… Evaluation Results for Twitter Test Set\")\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"AUC Score      : {roc_auc:.4f}\")\n",
        "print(f\"Precision (Fake): {precision:.4f}\")\n",
        "print(f\"Recall (Fake)  : {recall:.4f}\")\n",
        "print(f\"F1 Score (Fake): {f1:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DILy7v_M1nK"
      },
      "source": [
        "# Reddit test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO90dvBjM69v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xXrMTnlM9ti"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "test_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/New_test_set_test_features_reddit.csv\"\n",
        "test_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test/reddit\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/final_model1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "test_dataset = prepare_ensemble_data(test_csv, test_folder, transform, module1, module2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XYjFWaZNAtp",
        "outputId": "3f615ca9-7db0-484d-8c42-6fcf51743a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy reddit: 55.99%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the accuracy function\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Assuming EnsembleModel is defined and loaded correctly\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "# Load the trained weights from the saved model\n",
        "ensemble_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/ensemble_model_weights.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ensemble_model.eval()\n",
        "\n",
        "# Now compute the accuracy on the test set\n",
        "test_accuracy = compute_accuracy(ensemble_model, test_loader, device)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy reddit: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OzQJ7RmrKgR"
      },
      "source": [
        "### ROC, AUC and other performance measures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uZBya6pnvzf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "ensemble_model.eval()\n",
        "with torch.no_grad():\n",
        "    for module1_out, module2_out, labels in test_loader:\n",
        "        module1_out, module2_out = module1_out.to(device), module2_out.to(device)\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (fake)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# === ROC & AUC ===\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='magenta', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Deepfake Detection (Reddit Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=\"RdPu\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Deepfake Detection (Reddit Test Set)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# === Metrics ===\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, pos_label=1)\n",
        "recall = recall_score(all_labels, all_preds, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "print(f\"âœ… Evaluation Results for Reddit Test Set\")\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"AUC Score      : {roc_auc:.4f}\")\n",
        "print(f\"Precision (Fake): {precision:.4f}\")\n",
        "print(f\"Recall (Fake)  : {recall:.4f}\")\n",
        "print(f\"F1 Score (Fake): {f1:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrGluynCNB9T"
      },
      "source": [
        "# Facebook test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEyoarFNVIX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UUknbMMpNYR5"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "test_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/New_test_set_test_features_facebook.csv\"\n",
        "test_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test/facebook\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/final_model1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "test_dataset = prepare_ensemble_data(test_csv, test_folder, transform, module1, module2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z86wH00WNclx",
        "outputId": "28df4368-912b-4339-8f77-5289d03e89dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy facebook: 52.10%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the accuracy function\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Assuming EnsembleModel is defined and loaded correctly\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "# Load the trained weights from the saved model\n",
        "ensemble_model.load_state_dict(torch.load(\"/content/drive/MyDrive/HFMFmednytdataset/ensemble_model_weights.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ensemble_model.eval()\n",
        "\n",
        "# Now compute the accuracy on the test set\n",
        "test_accuracy = compute_accuracy(ensemble_model, test_loader, device)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy facebook: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVhY2mqXjhdc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "ensemble_model.eval()\n",
        "with torch.no_grad():\n",
        "    for module1_out, module2_out, labels in test_loader:\n",
        "        module1_out, module2_out = module1_out.to(device), module2_out.to(device)\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (fake)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# === ROC & AUC ===\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='magenta', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Deepfake Detection (Facebook Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=\"RdPu\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Deepfake Detection (Facebook Test Set)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# === Metrics ===\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, pos_label=1)\n",
        "recall = recall_score(all_labels, all_preds, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "print(f\"âœ… Evaluation Results for Facebook Test Set\")\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"AUC Score      : {roc_auc:.4f}\")\n",
        "print(f\"Precision (Fake): {precision:.4f}\")\n",
        "print(f\"Recall (Fake)  : {recall:.4f}\")\n",
        "print(f\"F1 Score (Fake): {f1:.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}