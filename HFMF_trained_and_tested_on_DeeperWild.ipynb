{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTfuak0RElsw"
      },
      "source": [
        "#Load of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQoVFezj6a3D"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84V_7_bK565p"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Using Modularized implementation of ViT -- ENGINE\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "  from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-WGDCVv6TLJ"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3Gv08V16XSQ"
      },
      "outputs": [],
      "source": [
        "set_seeds() # reproducibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmLhZkJCBEoM"
      },
      "source": [
        "#VIT model finetunes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6KgGN436pQL"
      },
      "outputs": [],
      "source": [
        "# Using pre-trained ViT base - 16\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "num_classes = 2\n",
        "# Freezing Base Parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gywa5Lpn6sOU"
      },
      "outputs": [],
      "source": [
        "# changing the ViT head -- ViT base has final embedding of 768\n",
        "pretrained_vit.heads = nn.Linear(in_features = pretrained_vit.heads.head.in_features, out_features = num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXo_vBN46vAb"
      },
      "outputs": [],
      "source": [
        "summary(model=pretrained_vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glv19Nje9Tte"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/Final_folder_code_thesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51uBy3sp6yVH"
      },
      "outputs": [],
      "source": [
        "from pathlib import PosixPath\n",
        "image_path = PosixPath(\"DeepWild_Final\")\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV2exOSm7Cg2"
      },
      "outputs": [],
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xpm02-s7FQy"
      },
      "outputs": [],
      "source": [
        "# Setup dataloaders\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=test_dir,\n",
        "                                                                                                     transform=pretrained_vit_transforms,\n",
        "                                                                                                     batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DJDqJ0b7Hep"
      },
      "outputs": [],
      "source": [
        "import going_modular.going_modular.engine\n",
        "print(going_modular.going_modular.engine.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyKxZCKC7Jg9"
      },
      "outputs": [],
      "source": [
        "from importlib import reload\n",
        "reload(going_modular.going_modular.engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE-Jsdwn7OcV"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of the pretrained ViT feature extractor model\n",
        "set_seeds()\n",
        "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
        "                                      train_dataloader=train_dataloader_pretrained,\n",
        "                                      test_dataloader=test_dataloader_pretrained,\n",
        "                                      optimizer=optimizer,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      epochs=10,\n",
        "                                      device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMzGfFYfJG77"
      },
      "outputs": [],
      "source": [
        "torch.save(pretrained_vit.state_dict(), \"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/deepwild_pretrained_vit_state_dict.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "d18KBTMVJaaW",
        "outputId": "152c5336-b455-403c-9d17-7a4c5722cbe8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the loss curves\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_vit_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19z-sSGzJiXr"
      },
      "outputs": [],
      "source": [
        "pretrained_vit_weights = torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/deepwild_pretrained_vit_state_dict.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwLnCJGDBOfR"
      },
      "source": [
        "#Res-Net50 model finetunes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIPyuzLxBUzD"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet-50 weights\n",
        "pretrained_resnet_weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
        "pretrained_resnet = torchvision.models.resnet50(weights=pretrained_resnet_weights).to(device)\n",
        "\n",
        "num_classes = 2  # Set the number of output classes\n",
        "\n",
        "# Freezing Base Parameters\n",
        "for parameter in pretrained_resnet.parameters():\n",
        "    parameter.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGjckLxjC8U9"
      },
      "outputs": [],
      "source": [
        "# changing the resnet head -- ViT base has final embedding of 768\n",
        "pretrained_resnet.fc = nn.Linear(in_features=pretrained_resnet.fc.in_features, out_features=num_classes).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhE4i7TTDOOZ"
      },
      "outputs": [],
      "source": [
        "summary(model=pretrained_resnet,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmOTpT4EDSAJ"
      },
      "outputs": [],
      "source": [
        "from pathlib import PosixPath\n",
        "image_path = PosixPath(\"DeepWild_Final\")\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsyMZC7gDmTf"
      },
      "outputs": [],
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_resnet_transforms = pretrained_resnet_weights.transforms()\n",
        "print(pretrained_resnet_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kVOOYbkD5yv"
      },
      "outputs": [],
      "source": [
        "# Setup dataloaders\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=test_dir,\n",
        "                                                                                                     transform=pretrained_resnet_transforms,\n",
        "                                                                                                     batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRMzMBOZEGh_"
      },
      "outputs": [],
      "source": [
        "import going_modular.going_modular.engine\n",
        "print(going_modular.going_modular.engine.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7fw7NyaEI5U"
      },
      "outputs": [],
      "source": [
        "from importlib import reload\n",
        "reload(going_modular.going_modular.engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMHDqKOjELZy"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_resnet.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of the pretrained ViT feature extractor model\n",
        "set_seeds()\n",
        "pretrained_resnet_results = engine.train(model=pretrained_resnet,\n",
        "                                      train_dataloader=train_dataloader_pretrained,\n",
        "                                      test_dataloader=test_dataloader_pretrained,\n",
        "                                      optimizer=optimizer,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      epochs=10,\n",
        "                                      device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcCnRJDgEVpc"
      },
      "outputs": [],
      "source": [
        "torch.save(pretrained_resnet.state_dict(), \"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/deepwild_pretrained_resnet_state_dict.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4jonEjpEaiz"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_resnet_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP6R9CKLEiRf"
      },
      "outputs": [],
      "source": [
        "pretrained_resnet_weights = torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/deepwild_pretrained_resnet_state_dict.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_drbJvJkKnve"
      },
      "source": [
        "#Feature extraction code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upMRruJ7Ktev"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "image_path = Path(\"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final\")\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"val\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNFpHcLELRF-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONi12ZJoLXTK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, limit=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        for label_folder in tqdm(['0_real', '1_fake'], desc=\"Loading dataset\"):\n",
        "            full_path = os.path.join(root_dir, label_folder)\n",
        "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
        "                if limit and idx >= limit:\n",
        "                    break  # Limit the number of files loaded\n",
        "                if file_name.endswith(('.jpg', '.png', '.jpeg','.JPEG','.JPG','.PNG')):  # Ensure image files\n",
        "                    self.image_files.append(os.path.join(full_path, file_name))\n",
        "                    if 'real' in label_folder:\n",
        "                        self.labels.append(0)  # Label 0 for real images\n",
        "                    else:\n",
        "                        self.labels.append(1)  # Label 1 for fake images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Paths to the training and validation directories\n",
        "train_dir = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/train\"\n",
        "val_dir = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/val\"\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = CustomDataset(root_dir=train_dir, transform=data_transforms)\n",
        "val_dataset = CustomDataset(root_dir=val_dir, transform=data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jelJxHZetE4Z"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "import certifi\n",
        "\n",
        "# Set the SSL context to use certifi's certificates\n",
        "ssl._create_default_https_context = ssl.create_default_context\n",
        "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0BXKVnZtHiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4_sX8aztPCI"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data transformations (ResNet-style transformations)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDatasetNew(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, limit=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        for label_folder in tqdm(['0_real', '1_fake'], desc=\"Loading dataset\"):\n",
        "            full_path = os.path.join(root_dir, label_folder)\n",
        "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
        "                if limit and idx >= limit:\n",
        "                    break  # Limit the number of files loaded\n",
        "                if file_name.endswith(('.jpg', '.png', '.jpeg','.JPEG','.JPG','.PNG')):  # Ensure image files\n",
        "                    self.image_files.append(os.path.join(full_path, file_name))\n",
        "                    if 'real' in label_folder:\n",
        "                        self.labels.append(0)  # Label 0 for real images\n",
        "                    else:\n",
        "                        self.labels.append(1)  # Label 1 for fake images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]  # This is a string path\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_path  # Ensure that the path returned is a string (not a tensor)\n",
        "\n",
        "\n",
        "# Load ResNet model and capture features\n",
        "def load_saved_resnet_model(model_path):\n",
        "    model = torchvision.models.resnet50(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # Freeze all layers\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 2)  # Binary classification (real/fake)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))  # Load the saved model\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Hook functions to capture low, mid, and high-level features\n",
        "    model.layer1[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, low_level_features))\n",
        "    model.layer3[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, mid_level_features))\n",
        "    model.layer4[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, high_level_features))\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Hook functions to capture ResNet features\n",
        "low_level_features, mid_level_features, high_level_features = [], [], []\n",
        "\n",
        "def hook_fn(module, input, output, storage_list):\n",
        "    storage_list.append(output.clone().detach())\n",
        "\n",
        "# Define linear layers to convert ResNet features to 768 dimensions\n",
        "# Define linear layers to convert ResNet features to 768 dimensions\n",
        "low_to_768 = nn.Linear(256, 768).to(device)   # For low-level features\n",
        "mid_to_768 = nn.Linear(1024, 768).to(device)  # For mid-level features\n",
        "high_to_768 = nn.Linear(2048, 768).to(device) # For high-level features\n",
        "\n",
        "def extract_resnet_features(model, image):\n",
        "    low_level_features.clear()\n",
        "    mid_level_features.clear()\n",
        "    high_level_features.clear()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "        model(image)\n",
        "\n",
        "    # Pool ResNet features and map to 768 dimensions\n",
        "    low_pooled = F.adaptive_avg_pool2d(low_level_features[-1].to(device), (1, 1)).squeeze()\n",
        "    mid_pooled = F.adaptive_avg_pool2d(mid_level_features[-1].to(device), (1, 1)).squeeze()\n",
        "    high_pooled = F.adaptive_avg_pool2d(high_level_features[-1].to(device), (1, 1)).squeeze()\n",
        "\n",
        "    low_768 = low_to_768(low_pooled)   # Shape [1, 768]\n",
        "    mid_768 = mid_to_768(mid_pooled)   # Shape [1, 768]\n",
        "    high_768 = high_to_768(high_pooled) # Shape [1, 768]\n",
        "\n",
        "    return low_768, mid_768, high_768\n",
        "\n",
        "\n",
        "# Function to preprocess the image using ViT's transforms\n",
        "def pipeline_preprocessor():\n",
        "    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "    return vit_weights.transforms()\n",
        "\n",
        "# Function to extract ViT embeddings\n",
        "def get_vit_embedding(vit_model, image_path):\n",
        "    preprocessing = pipeline_preprocessor()  # Preprocessing from ViT\n",
        "    img = Image.open(image_path).convert(\"RGB\")  # Ensure we load image by path (string)\n",
        "    img = preprocessing(img).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = vit_model._process_input(img)\n",
        "        batch_class_token = vit_model.class_token.expand(img.shape[0], -1, -1)\n",
        "        feats = torch.cat([batch_class_token, feats], dim=1)\n",
        "        feats = vit_model.encoder(feats)\n",
        "        vit_hidden = feats[:, 0]  # CLS token\n",
        "    return vit_hidden\n",
        "\n",
        "# Load ViT model\n",
        "def load_vit_model(pretrained_weights_path):\n",
        "    vit_model = torchvision.models.vit_b_16(pretrained=False).to(device)\n",
        "    pretrained_vit_weights = torch.load(pretrained_weights_path, map_location=device)\n",
        "    vit_model.load_state_dict(pretrained_vit_weights, strict=False)\n",
        "    vit_model.eval()  # Set to evaluation mode\n",
        "    return vit_model\n",
        "\n",
        "# Add a sequence dimension (if missing) before applying attention\n",
        "def ensure_correct_shape(tensor):\n",
        "    if len(tensor.shape) == 2:  # If shape is [batch_size, embedding_dim]\n",
        "        tensor = tensor.unsqueeze(1)  # Add a sequence dimension: [batch_size, 1, embedding_dim]\n",
        "    elif len(tensor.shape) == 1:  # If shape is [embedding_dim]\n",
        "        tensor = tensor.unsqueeze(0).unsqueeze(1)  # Add batch and sequence dimensions: [1, 1, embedding_dim]\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# Scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    # Ensure Q, K, and V have the correct shapes\n",
        "    Q = ensure_correct_shape(Q)  # Should be [batch_size, 1, embedding_dim]\n",
        "    K = ensure_correct_shape(K)  # Should be [batch_size, 1, embedding_dim]\n",
        "    V = ensure_correct_shape(V)  # Should be [batch_size, 1, embedding_dim]\n",
        "\n",
        "#     print(f\"Q shape after unsqueeze: {Q.shape}, K shape after unsqueeze: {K.shape}, V shape after unsqueeze: {V.shape}\")  # Debugging\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32).to(Q.device))\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn_weights, V)\n",
        "    return output\n",
        "\n",
        "# Save features for each dataset (train/val/test)\n",
        "import csv\n",
        "\n",
        "# Save features for each dataset (train/val/test) as CSV\n",
        "def save_features_to_csv(model, vit_model, data_loader, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    with open(save_path, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write the CSV header\n",
        "        writer.writerow([\"image_name\", \"features\", \"label\"])\n",
        "\n",
        "        for images, img_paths in tqdm(data_loader, desc=\"Extracting features\"):\n",
        "            for i in range(len(images)):\n",
        "                image = images[i].to(device)  # Move image to the correct device\n",
        "                img_path = img_paths[i]  # Image path\n",
        "\n",
        "                # Ensure img_path is a string\n",
        "                if isinstance(img_path, torch.Tensor):\n",
        "                    img_path = img_path.item() if img_path.dim() == 0 else str(img_path)\n",
        "\n",
        "                # Extract ResNet features\n",
        "                try:\n",
        "                    low_768, mid_768, high_768 = extract_resnet_features(model, image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting ResNet features for {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract ViT features\n",
        "                try:\n",
        "                    vit_hidden = get_vit_embedding(vit_model, img_path)  # img_path should be a string\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting ViT features for {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Apply attention between ResNet and ViT features\n",
        "                try:\n",
        "                    output_1 = scaled_dot_product_attention(vit_hidden, low_768, low_768)\n",
        "                    output_2 = scaled_dot_product_attention(output_1, mid_768, mid_768)\n",
        "                    final_output = scaled_dot_product_attention(output_2, high_768, high_768)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error applying attention for {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert features to a flattened list\n",
        "                features = final_output.detach().cpu().numpy().flatten().tolist()\n",
        "\n",
        "\n",
        "                # Extract label from the image path\n",
        "                label = 0 if \"real\" in img_path else 1\n",
        "\n",
        "                # Write the row to the CSV\n",
        "                writer.writerow([os.path.basename(img_path), features, label])\n",
        "\n",
        "    print(f\"Features saved to {save_path}\")\n",
        "\n",
        "\n",
        "# Load models\n",
        "resnet_model = load_saved_resnet_model('/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/deepwild_pretrained_resnet_state_dict.pth')\n",
        "vit_model = load_vit_model('/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/deepwild_pretrained_vit_state_dict.pth')\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/train\"\n",
        "val_dir = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/val\"\n",
        "test_dir=\"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test\"\n",
        "\n",
        "train_dataset = CustomDatasetNew(root_dir=train_dir, transform=data_transforms)\n",
        "val_dataset = CustomDatasetNew(root_dir=val_dir, transform=data_transforms)\n",
        "#test_dataset = CustomDatasetNew(root_dir=test_dir, transform=data_transforms)\n",
        "for test_subdir in ['twitter', 'facebook', 'reddit']:\n",
        "    test_dataset = CustomDatasetNew(root_dir=os.path.join(test_dir, test_subdir), transform=data_transforms)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    print(f\"Processing Test Dataset: {test_subdir}\")\n",
        "\n",
        "    # Save features to a separate CSV file for each subdirectory\n",
        "    save_path = f\"features_deepwild/test_features_{test_subdir}.csv\"\n",
        "    save_features_to_csv(resnet_model, vit_model, test_loader, save_path=save_path)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "print(\"Processing Train Dataset:\")\n",
        "save_features_to_csv(resnet_model, vit_model, train_loader, save_path=\"features_deepwild/train_features.csv\")\n",
        "\n",
        "print(\"Processing Validation Dataset:\")\n",
        "save_features_to_csv(resnet_model, vit_model, val_loader, save_path=\"features_deepwild/val_features.csv\")\n",
        "\n",
        "print(\"Processing Test Dataset Face:\")\n",
        "save_features_to_csv(resnet_model, vit_model, test_loader, save_path=\"features_deepwild/test_features.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7I6PTi1TDWT"
      },
      "source": [
        "#Building the overall module 1 model and training it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THQoYUauTCsL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter1d  # Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBq0MmyDTk2Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP0IaRg-UMEG"
      },
      "outputs": [],
      "source": [
        "#setting seed in both numpy and torch\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5WhgViBUP3F"
      },
      "outputs": [],
      "source": [
        "#defining the function get_req_set which basically loads in the feature from above and names them, make them into tensors and create a dataset with that, wereafter a dataloader is created for a model.\n",
        "\n",
        "def get_req_set(path):\n",
        "  df = pd.read_csv(path)\n",
        "  features_df = df['features'].str.strip('[]').str.split(',', expand=True)\n",
        "  features_df = features_df.astype(float)\n",
        "  features_df.columns = [f'feature_{i}' for i in range(features_df.shape[1])]\n",
        "  df_expanded = pd.concat([features_df, df['label']], axis=1)\n",
        "  X = df_expanded.drop(columns=['label'])\n",
        "  y = df_expanded['label']\n",
        "  X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "  y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
        "  dataset = TensorDataset(X_tensor, y_tensor)\n",
        "  print(len(dataset))\n",
        "  temp_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "  return temp_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifQMG7eTVJlm"
      },
      "outputs": [],
      "source": [
        "train_loader = get_req_set('/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/train_features.csv')\n",
        "val_loader_1 = get_req_set('/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/test_features_twitter.csv')\n",
        "val_loader_2 = get_req_set('/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/test_features_reddit.csv')\n",
        "val_loader_3 = get_req_set('/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/test_features_facebook.csv')\n",
        "val_loader_4 = get_req_set('/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/val_features.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRz9ZErpWgBO"
      },
      "outputs": [],
      "source": [
        "#creation of the Deep Neural Network.\n",
        "#init initializes the class and afterwards you define what the class should look like\n",
        "#the layers of the model are then defined\n",
        "#lastly the method of which the data is pass through the model is defined. In this case it is forward pass, in a sequential manner.\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plZ93dAxXfN3"
      },
      "outputs": [],
      "source": [
        "# Updated loop with 2 validation datasets and additional metrics\n",
        "input_dim = 768      # Number of features in the produced dataset\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 256\n",
        "output_dim = 2 # Number of classes -- 2\n",
        "model = DNN(input_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        outputs = model(batch_X)\n",
        "        #print(outputs)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == batch_y).sum().item()\n",
        "        total_samples += batch_y.size(0)\n",
        "\n",
        "    # Calculate train loss and metrics\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct_predictions / total_samples * 100\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on train data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_true_train = []\n",
        "        y_pred_train = []\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true_train.extend(batch_y.cpu().numpy())\n",
        "            y_pred_train.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate train metrics\n",
        "    train_precision = precision_score(y_true_train, y_pred_train, average='binary')\n",
        "    train_recall = recall_score(y_true_train, y_pred_train, average='binary')\n",
        "    train_f1 = f1_score(y_true_train, y_pred_train, average='binary')\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Train Precision: {train_precision:.2f}, Train Recall: {train_recall:.2f}, Train F1: {train_f1:.2f}\")\n",
        "\n",
        "    # Validation loaders and names\n",
        "    val_loaders = [val_loader_1, val_loader_2, val_loader_3, val_loader_4]\n",
        "    val_names = ['Twitter','Reddit','Facebook','Validation']\n",
        "\n",
        "    # Evaluate on each validation set\n",
        "    for val_loader, val_name in zip(val_loaders, val_names):\n",
        "        y_true_val = []\n",
        "        y_pred_val = []\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                y_true_val.extend(batch_y.cpu().numpy())\n",
        "                y_pred_val.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics for each validation set\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = accuracy_score(y_true_val, y_pred_val) * 100\n",
        "        val_precision = precision_score(y_true_val, y_pred_val, average='binary')\n",
        "        val_recall = recall_score(y_true_val, y_pred_val, average='binary')\n",
        "        val_f1 = f1_score(y_true_val, y_pred_val, average='binary')\n",
        "\n",
        "        print(f\"{val_name} - \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, \"\n",
        "              f\"Val Precision: {val_precision:.2f}, Val Recall: {val_recall:.2f}, Val F1: {val_f1:.2f}\")\n",
        "\n",
        "    print(\"------------------------------------------------\")\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/final_model_1_weights.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQBmqPS7z5sI"
      },
      "source": [
        "#Evaluating module 1 model and calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWMyz2oeZyHZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#load the model\n",
        "#and evaluate it\n",
        "\n",
        "#outcomment the below if used from previous created model\n",
        "'''input_dim = 768      # Number of features in the produced dataset\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 256\n",
        "output_dim = 2 # Number of classes -- 2\n",
        "model = DNN(input_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "num_epochs = 10'''\n",
        "\n",
        "model = DNN(input_dim, hidden_dim_1, hidden_dim_2, output_dim)  # Replace with your model class\n",
        "model.load_state_dict(torch.load('//content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/final_model_1_weights.pth'))  # Load saved weights\n",
        "model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJQdsU-bZ8AA"
      },
      "outputs": [],
      "source": [
        "#creates two empty lists for future use.\n",
        "\n",
        "softmax_probs = []\n",
        "true_labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwPxVL2OZ-Uq"
      },
      "outputs": [],
      "source": [
        "#tells it not to do backpropagation, since we are evaluating the model. Saves time.\n",
        "#afterwards, it loads the dataloader, which will feed data into the evaluator.\n",
        "#next is passes the inputs through the model, to get the final prediction.\n",
        "#the softmax converts the score into probabilities, which we can use to determine whether it is a fake or not.\n",
        "#next the probabilities and the true labels are added into the lists, to see whether the guesses are right or not.\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in train_loader:  # Iterate over batches\n",
        "        logits = model(inputs)  # Get the model's raw predictions (logits)\n",
        "        probs = F.softmax(logits, dim=1)  # Apply softmax to get probabilities\n",
        "        softmax_probs.append(probs.cpu().numpy())  # Append probabilities to list\n",
        "        true_labels.append(labels.cpu().numpy())  # Append true labels to list\n",
        "\n",
        "# Once the loop is done, convert the lists to NumPy arrays\n",
        "softmax_probs = np.concatenate(softmax_probs, axis=0)  # Concatenate all probabilities\n",
        "true_labels = np.concatenate(true_labels, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npfBclDsEolM"
      },
      "outputs": [],
      "source": [
        "#feines the plot calibration curve\n",
        "\n",
        "def plot_calibration_curve(y_true, y_prob, n_bins=10, label='Uncalibrated'):\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label=label)\n",
        "    return prob_true, prob_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCYvEYT2bNBc"
      },
      "outputs": [],
      "source": [
        "#This code is plotting a calibration curve for the positive class in a binary classification setting.\n",
        "#What we are actually seeing is how well the predicted probabilities align with the actual outcomes (meaning the labels) for the positive class (meaning real ones (label 1))\n",
        "\n",
        "\n",
        "from sklearn.calibration import calibration_curve\n",
        "positive_probs = softmax_probs[:, 1]\n",
        "positive_labels = (true_labels == 1).astype(int)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_calibration_curve(positive_labels, positive_probs, label='Uncalibrated')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve (Uncalibrated for Positive Class)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "positive_probs = softmax_probs[:, 1]\n",
        "positive_labels = (true_labels == 1).astype(int)\n",
        "\n",
        "# Get the calibration curve data\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(positive_labels, positive_probs, n_bins=10)\n",
        "\n",
        "# Plot the calibration curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(mean_predicted_value, fraction_of_positives, label='Uncalibrated')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')  # Perfect calibration line\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve (Uncalibrated for Positive Class)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnCD-4cJczny"
      },
      "outputs": [],
      "source": [
        "#defines that logits (predictions) are the softmax probabilities\n",
        "\n",
        "logits = softmax_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6q6UCGOc3DN"
      },
      "outputs": [],
      "source": [
        "# Assuming `softmax_probs` are already probabilities in [0, 1] (logits transformed by softmax)\n",
        "positive_class_idx = 1  # Define the positive class index\n",
        "positive_probs = softmax_probs[:, positive_class_idx]  # Extract positive class probabilities\n",
        "positive_labels = (true_labels == positive_class_idx).astype(int)  # Binary labels for the positive class\n",
        "\n",
        "# Fit PLATT SCALING -- scaled Logistic Regression to logits\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(positive_probs.reshape(-1,1), true_labels)  # Fit on positive class probabilities and binary labels\n",
        "\n",
        "# Predict calibrated probabilities for the positive class\n",
        "calibrated_probs_iso = log_reg.predict_proba(positive_probs.reshape(-1,1))[:,1]\n",
        "\n",
        "# Plot Calibration Curve\n",
        "def plot_calibration_curve(y_true, y_prob, n_bins=10, label=None):\n",
        "    from sklearn.calibration import calibration_curve\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label=label)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Uncalibrated\n",
        "plot_calibration_curve(true_labels, positive_probs, label='Uncalibrated')\n",
        "\n",
        "# Calibrated\n",
        "plot_calibration_curve(true_labels, calibrated_probs_iso, label='Platt Scaling')\n",
        "\n",
        "# Perfect calibration line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "\n",
        "# Plot settings\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve for Positive Class (Isotonic Regression)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nld5vhicdKfw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# checking uncalibrated model on VAL\n",
        "softmax_probs = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader_4: #val\n",
        "        logits = model(inputs)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        softmax_probs.append(probs.cpu().numpy())\n",
        "        true_labels.append(labels.cpu().numpy())\n",
        "softmax_probs = np.concatenate(softmax_probs, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "logits = softmax_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl4E_YnNdQFB"
      },
      "outputs": [],
      "source": [
        "# Assuming `softmax_probs` are already probabilities in [0, 1] (logits transformed by softmax)\n",
        "positive_class_idx = 1  # Define the positive class index\n",
        "positive_probs = softmax_probs[:, positive_class_idx]  # Extract positive class probabilities\n",
        "positive_labels = (true_labels == positive_class_idx).astype(int)  # Binary labels for the positive class\n",
        "\n",
        "calibrated_probs_iso = log_reg.predict(positive_probs.reshape(-1,1))\n",
        "\n",
        "# Plot Calibration Curve\n",
        "def plot_calibration_curve(y_true, y_prob, n_bins=10, label=None):\n",
        "    from sklearn.calibration import calibration_curve\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label=label)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Uncalibrated\n",
        "plot_calibration_curve(true_labels, positive_probs, label='Uncalibrated')\n",
        "\n",
        "# Calibrated\n",
        "plot_calibration_curve(true_labels, calibrated_probs_iso, label='Isotonic Regression')\n",
        "\n",
        "# Perfect calibration line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "\n",
        "# Plot settings\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve for Positive Class (Isotonic Regression)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxxg9-9odXKj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_ece(y_true, y_prob, n_bins=600):\n",
        "    bins = np.linspace(0, 1, n_bins + 1)  # Define bin edges\n",
        "    bin_indices = np.digitize(y_prob, bins) - 1  # Assign probabilities to bins\n",
        "\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        bin_mask = bin_indices == i\n",
        "        bin_count = np.sum(bin_mask)\n",
        "        if bin_count > 0:\n",
        "            bin_confidence = np.mean(y_prob[bin_mask])  # Mean predicted probability\n",
        "            bin_accuracy = np.mean(y_true[bin_mask])  # Fraction of positives\n",
        "            ece += (bin_count / len(y_prob)) * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "    return ece\n",
        "\n",
        "# Compute ECE for uncalibrated and calibrated models\n",
        "ece_uncalibrated = compute_ece(true_labels, positive_probs)\n",
        "ece_calibrated = compute_ece(true_labels, calibrated_probs_iso)\n",
        "\n",
        "print(f\"ECE (Uncalibrated) on Val: {ece_uncalibrated:.4f}\")\n",
        "print(f\"ECE (Calibrated): on Val {ece_calibrated:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0O8idAjFHUq"
      },
      "source": [
        "#Module 2, training of Sobel, YOLO, FaceMesh and Xception models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLe-eS-cX_Jd"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QtLQ3VWXv5n"
      },
      "outputs": [],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtPC3_IcXyYF"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==5.26.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oWS_85VYMCV"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaTbHFHAHq5k"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install mediapipe==0.10.21\n",
        "!pip install pretrainedmodels\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkAWXPn3Hf6Y"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "import ssl\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acBEiZgvIcug"
      },
      "outputs": [],
      "source": [
        "#this block of code defines the models and the tranformations for the pictures to fit into the model and yield the answer we want (which is a combined answer of fake or real dependant on all the models answers)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for Xception\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((299, 299)),  # Xception requires 299x299 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "# Loading YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "# Dataset Class\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, limit=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self._load_data(limit)\n",
        "\n",
        "    def _load_data(self, limit):\n",
        "        if self.split == 'test':\n",
        "            for platform in ['facebook', 'reddit', 'twitter']:\n",
        "                for label in ['0_real', '1_fake']:\n",
        "                    path = os.path.join(self.root_dir, self.split, platform, label)\n",
        "                    self._load_images(path, label, limit)\n",
        "        else:\n",
        "            for label in ['0_real', '1_fake']:\n",
        "                path = os.path.join(self.root_dir, self.split, label)\n",
        "                self._load_images(path, label, limit)\n",
        "\n",
        "    def _load_images(self, path, label, limit):\n",
        "        if os.path.exists(path):\n",
        "            for i, img_name in enumerate(os.listdir(path)):\n",
        "                if limit and len(self.data) >= limit:\n",
        "                    break\n",
        "                self.data.append(os.path.join(path, img_name))\n",
        "                self.labels.append(0 if label == '0_real' else 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx]\n",
        "        image = cv2.imread(img_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # YOLOv8 for object detection\n",
        "        results = yolo_model(image)\n",
        "        detected_objects = []\n",
        "        face_landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "        for result in results[0].boxes:\n",
        "            class_id = int(result.cls[0])  # YOLOv8 returns class IDs\n",
        "            # class_name = YOLO.names[class_id]  # Get class name from YOLO COCO classes\n",
        "            class_name = yolo_model.names[class_id]\n",
        "\n",
        "            # Check if the detected object is one of the COCO classes we care about\n",
        "            if class_name in COCO_CLASSES:\n",
        "                x1, y1, x2, y2 = result.xyxy[0].cpu().numpy()\n",
        "                obj_crop = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "                detected_objects.append(class_id)\n",
        "\n",
        "                # If the detected object is a person, get facial landmarks\n",
        "                if class_name == 'person':\n",
        "                    results_face = face_mesh.process(cv2.cvtColor(obj_crop, cv2.COLOR_BGR2RGB))\n",
        "                    if results_face.multi_face_landmarks:\n",
        "                        face_landmarks = np.array([[p.x, p.y] for p in results_face.multi_face_landmarks[0].landmark])\n",
        "                        face_landmarks = face_landmarks.flatten()\n",
        "\n",
        "        # Encode detected objects as a one-hot vector of COCO class detections\n",
        "        yolo_features = np.zeros(len(COCO_CLASSES))\n",
        "        for obj_id in detected_objects:\n",
        "            yolo_features[obj_id] = 1  # Mark the detected class in the one-hot vector\n",
        "\n",
        "        # Applying Sobel edge detection\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "        sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "        sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "        # Transform the image for Xception model\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            sobel_combined = self.transform(sobel_combined)\n",
        "\n",
        "        yolo_features = torch.tensor(yolo_features, dtype=torch.float32).to(device)\n",
        "        face_landmarks = torch.tensor(face_landmarks, dtype=torch.float32).to(device)\n",
        "\n",
        "        return image.to(device), sobel_combined.to(device), yolo_features, face_landmarks, torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "# Define the Classifier Model using Xception\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5gA_sPMKOOF"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "\n",
        "# Dataset and DataLoader\n",
        "root_dir = '/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final'  # Replace with actual path\n",
        "train_dataset = DeepfakeDataset(root_dir=root_dir, split='train', transform=transform)\n",
        "val_dataset = DeepfakeDataset(root_dir=root_dir, split='val', transform=transform)\n",
        "test_dataset = DeepfakeDataset(root_dir=root_dir, split='test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define model, loss function, and optimizer\n",
        "model = DeepfakeClassifier().to(device)\n",
        "# Dynamically calculate the flattened size for sobel_cnn\n",
        "model.initialize_sobel_linear(input_shape=(3, 299, 299))  # Assuming Sobel image size is 299x299\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Hyperparameter (tunable)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Stop if no improvement in validation loss after 5 epochs\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Lists to store accuracy and loss values for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_model_path = \"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth\"\n",
        "\n",
        "# Training Loop with validation and early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, sobel_images, yolo_features, landmarks, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, sobel_images, yolo_features, landmarks)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_loader)\n",
        "    epoch_train_accuracy = correct_train / total_train\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, sobel_images, yolo_features, landmarks, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
        "            outputs = model(images, sobel_images, yolo_features, landmarks)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_val += (preds == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_loader)\n",
        "    epoch_val_accuracy = correct_val / total_val\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_accuracy * 100:.2f}%, \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_val_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Early Stopping and Model Selection\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
        "        print(f\"Best model saved with validation loss: {best_val_loss:.4f}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load the best model for final testing or further evaluation\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(\"Loaded the best model based on validation performance.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMCCAE5cuW3P"
      },
      "outputs": [],
      "source": [
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"training_validation_accuracy_module1_WildRf.png\")\n",
        "print(\"Training and Validation accuracy plot saved as 'training_validation_accuracy_module1_WildRf.png'.\")\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"training_validation_loss_module1_WildRf.png\")\n",
        "print(\"Training and Validation loss plot saved as 'training_validation_loss_module1_WildRf.png'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCPQs-7u2Y3"
      },
      "source": [
        "#Testing on module 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VEeML4vu5DT"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "#remember to also load the test set loader and the photos if you are running this with the saved model\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth'))\n",
        "model.eval()  # Set to evaluation mode\n",
        "print(\"Loaded the best model for testing.\")\n",
        "\n",
        "# Test Phase\n",
        "running_test_loss = 0.0\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, sobel_images, yolo_features, landmarks, labels in tqdm(test_loader, desc=\"Testing on Test Set\"):\n",
        "        outputs = model(images, sobel_images, yolo_features, landmarks)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_test_loss += loss.item()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_test += (preds == labels).sum().item()\n",
        "        total_test += labels.size(0)\n",
        "\n",
        "test_loss = running_test_loss / len(test_loader)\n",
        "test_accuracy = correct_test / total_test\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfgM8RrmzBnJ"
      },
      "outputs": [],
      "source": [
        "!pip install torchcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkM8cHbp1c5e"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na7qdQfQyhxr"
      },
      "outputs": [],
      "source": [
        "root_dir = '/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final'  # Replace with actual path\n",
        "test_dataset = DeepfakeDataset(root_dir=root_dir, split='test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "from torchcam.methods import GradCAM\n",
        "# Define Model\n",
        "model = DeepfakeClassifier().to(device)\n",
        "\n",
        "# Dynamically calculate the flattened size for sobel_cnn\n",
        "sobel_input_shape = (3, 299, 299)  # Assuming Sobel image size is 299x299\n",
        "model.initialize_sobel_linear(input_shape=sobel_input_shape)\n",
        "\n",
        "# Load pretrained weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Use TorchCAM's GradCAM\n",
        "# Replace with the correct convolutional layer from your model\n",
        "target_layer = \"xception.block5.rep.4.pointwise\"  # Example convolutional layer\n",
        "grad_cam = GradCAM(model, target_layer)\n",
        "import os\n",
        "\n",
        "def gradcam_visualization_on_fake_images(model, loader, grad_cam, num_images=5, save_dir=\"gradcam_outputs\"):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    images_processed = 0\n",
        "\n",
        "    # Create directory to save Grad-CAM outputs\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for images, sobel_images, yolo_features, landmarks, labels in loader:\n",
        "        images, sobel_images, yolo_features, landmarks, labels = (\n",
        "            images.to(device),\n",
        "            sobel_images.to(device),\n",
        "            yolo_features.to(device),\n",
        "            landmarks.to(device),\n",
        "            labels.to(device),\n",
        "        )\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            # Process only fake images (label == 1)\n",
        "            if labels[i].item() != 1:\n",
        "                continue  # Skip non-fake images\n",
        "\n",
        "            if images_processed >= num_images:\n",
        "                return  # Stop after visualizing `num_images`\n",
        "\n",
        "            input_image = images[i].unsqueeze(0)\n",
        "            sobel_image = sobel_images[i].unsqueeze(0)\n",
        "            yolo_feature = yolo_features[i].unsqueeze(0)\n",
        "            landmark = landmarks[i].unsqueeze(0)\n",
        "\n",
        "            # Forward pass to get predictions\n",
        "            outputs = model(input_image, sobel_image, yolo_feature, landmark)\n",
        "            pred_class = outputs.argmax(dim=1).item()\n",
        "\n",
        "            print(f\"Fake Image {images_processed + 1} - Predicted Class: {pred_class}, True Label: {labels[i].item()}\")\n",
        "\n",
        "            # Generate Grad-CAM heatmap\n",
        "            activation_map = grad_cam(class_idx=pred_class, scores=outputs)  # Explicitly pass class_idx and scores\n",
        "\n",
        "            # Remove batch dimension for visualization\n",
        "            heatmap = activation_map[0].squeeze().cpu().numpy()  # Shape: (19, 19)\n",
        "\n",
        "            # Resize heatmap to match input image dimensions\n",
        "            heatmap_resized = cv2.resize(heatmap, (299, 299))  # Assuming the input image size is 299x299\n",
        "\n",
        "            # Normalize heatmap for better visualization\n",
        "            heatmap_resized = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min())\n",
        "\n",
        "            input_image_vis = to_pil_image(input_image.squeeze().cpu())\n",
        "\n",
        "            # Plot and save the images\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            # Original Image\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(input_image_vis)\n",
        "            plt.title(\"Original Image\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            # Heatmap\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(heatmap_resized, cmap=\"jet\")\n",
        "            plt.title(\"Grad-CAM Heatmap\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            # Overlayed Image\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(input_image_vis)\n",
        "            plt.imshow(heatmap_resized, cmap=\"jet\", alpha=0.5)  # Overlay heatmap\n",
        "            plt.title(\"Overlay\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            # Save the figure\n",
        "            output_path = os.path.join(save_dir, f\"gradcam_fake_output_{images_processed + 1}.png\")\n",
        "            plt.savefig(output_path, bbox_inches=\"tight\")\n",
        "            print(f\"Saved Grad-CAM visualization for fake image as {output_path}\")\n",
        "\n",
        "            plt.close()\n",
        "            images_processed += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply Grad-CAM only on fake images from the test dataset\n",
        "gradcam_visualization_on_fake_images(model, test_loader, grad_cam, num_images=20, save_dir=\"/content/drive/MyDrive/Final_folder_code_thesis/gradcam_fake_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4rchIsU31fj"
      },
      "source": [
        "#Combining the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeExS5rV31E7"
      },
      "outputs": [],
      "source": [
        "#load packages for combining models\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gFuiLTyDfXn9"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "train_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/train_features.csv\"\n",
        "train_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/train\"\n",
        "\n",
        "val_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/val_features.csv\"\n",
        "val_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/val\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/final_model_1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_dataset = prepare_ensemble_data(train_csv, train_folder, transform, module1, module2)\n",
        "val_dataset = prepare_ensemble_data(val_csv, val_folder, transform, module1, module2)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxV27LA7VzH"
      },
      "source": [
        "#Training the combined models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iymKcWUzE8L9"
      },
      "outputs": [],
      "source": [
        "#Define the accuracy function\n",
        "\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "''''\n",
        "# Compute accuracy\n",
        "train_accuracy = compute_accuracy(ensemble_model, train_loader, device)\n",
        "val_accuracy = compute_accuracy(ensemble_model, val_loader, device)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Initial Training Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Initial Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "'''\n",
        "\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    ensemble_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for module1_out, module2_out, labels in train_loader:\n",
        "        module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_accuracy = compute_accuracy(ensemble_model, train_loader, device)\n",
        "val_accuracy = compute_accuracy(ensemble_model, val_loader, device)\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5YrKv__G4qK"
      },
      "outputs": [],
      "source": [
        "#save the trained ensemble model\n",
        "\n",
        "torch.save(ensemble_model.state_dict(), \"/content/drive/MyDrive/Final_folder_code_thesis/Original_model/ensemble_model_weights.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buS4XZ7bXKaW"
      },
      "source": [
        "\n",
        "#FACEBOOK TEST DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78DdmUa4Yol6"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixmCOF4pYol9"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==5.26.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kamMZ-9iYol-"
      },
      "outputs": [],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gOA4VgdYol_"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FdTu4mVYomC"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install mediapipe==0.10.21\n",
        "!pip install pretrainedmodels\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwThTNdfXTLm"
      },
      "outputs": [],
      "source": [
        "#load packages for combining models\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ7Xz4CzXZPH"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "test_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/test_features_facebook.csv\"\n",
        "test_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test/facebook\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/final_model_1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "test_dataset = prepare_ensemble_data(test_csv, test_folder, transform, module1, module2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxylqdfEYv7R",
        "outputId": "54338164-782a-4cd4-df95-97bc28e71081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy facebook: 88.62%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the accuracy function\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Assuming EnsembleModel is defined and loaded correctly\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "# Load the trained weights from the saved model\n",
        "ensemble_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/ensemble_model_weights.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ensemble_model.eval()\n",
        "\n",
        "# Now compute the accuracy on the test set\n",
        "test_accuracy = compute_accuracy(ensemble_model, test_loader, device)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy facebook: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "ensemble_model.eval()\n",
        "with torch.no_grad():\n",
        "    for module1_out, module2_out, labels in test_loader:\n",
        "        module1_out, module2_out = module1_out.to(device), module2_out.to(device)\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (fake)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# === ROC & AUC ===\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='magenta', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Deepfake Detection (Facebook Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=\"RdPu\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Deepfake Detection (Facebook Test Set)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# === Metrics ===\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, pos_label=1)\n",
        "recall = recall_score(all_labels, all_preds, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "print(f\"✅ Evaluation Results for Facebook Test Set\")\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"AUC Score      : {roc_auc:.4f}\")\n",
        "print(f\"Precision (Fake): {precision:.4f}\")\n",
        "print(f\"Recall (Fake)  : {recall:.4f}\")\n",
        "print(f\"F1 Score (Fake): {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CQDoajzTle88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6QLtuOhkfW4"
      },
      "source": [
        "#REDDIT TEST DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgsEAXQYkfW5"
      },
      "outputs": [],
      "source": [
        "#load packages for combining models\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhdEx7nRkfW6"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "test_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/test_features_reddit.csv\"\n",
        "test_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test/reddit\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/final_model_1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "test_dataset = prepare_ensemble_data(test_csv, test_folder, transform, module1, module2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0J5eQr3kfW7",
        "outputId": "45f57139-0951-436e-b58b-50a1ba4f7d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy reddit: 87.72%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the accuracy function\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Assuming EnsembleModel is defined and loaded correctly\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "# Load the trained weights from the saved model\n",
        "ensemble_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/ensemble_model_weights.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ensemble_model.eval()\n",
        "\n",
        "# Now compute the accuracy on the test set\n",
        "test_accuracy = compute_accuracy(ensemble_model, test_loader, device)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy reddit: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "ensemble_model.eval()\n",
        "with torch.no_grad():\n",
        "    for module1_out, module2_out, labels in test_loader:\n",
        "        module1_out, module2_out = module1_out.to(device), module2_out.to(device)\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (fake)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# === ROC & AUC ===\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='magenta', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Deepfake Detection (Reddit Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=\"RdPu\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Deepfake Detection (Reddit Test Set)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# === Metrics ===\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, pos_label=1)\n",
        "recall = recall_score(all_labels, all_preds, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "print(f\"✅ Evaluation Results for Reddit Test Set\")\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"AUC Score      : {roc_auc:.4f}\")\n",
        "print(f\"Precision (Fake): {precision:.4f}\")\n",
        "print(f\"Recall (Fake)  : {recall:.4f}\")\n",
        "print(f\"F1 Score (Fake): {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RKKbyHMflh5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soc8pzd8lya_"
      },
      "source": [
        "#TWITTER TEST DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f83L75lHlybB"
      },
      "outputs": [],
      "source": [
        "#load packages for combining models\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from ultralytics import YOLO  # For YOLOv8 face detection\n",
        "from tqdm import tqdm\n",
        "import pretrainedmodels  # For Xception model\n",
        "import numpy as np\n",
        "import mediapipe as mp  # For facial landmark extraction\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FglidOSmlybB"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "test_csv = \"/content/drive/MyDrive/Final_folder_code_thesis/features_deepwild/test_features_twitter.csv\"\n",
        "test_folder = \"/content/drive/MyDrive/Final_folder_code_thesis/DeepWild_Final/test/twitter\"\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class DeepfakeClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepfakeClassifier, self).__init__()\n",
        "        self.xception = xception_model  # Outputs 128 features\n",
        "        self.sobel_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.sobel_linear = None  # Will initialize dynamically\n",
        "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
        "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
        "        self.fc1 = None  # To be initialized dynamically\n",
        "        self.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "    def initialize_sobel_linear(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            # Initialize Sobel Linear\n",
        "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
        "            output = self.sobel_cnn(sample_input)\n",
        "            flattened_size = output.view(-1).size(0)\n",
        "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
        "\n",
        "            # Calculate the total feature size for fc1\n",
        "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
        "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
        "\n",
        "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
        "        # Process features\n",
        "        yolo_features = yolo_features.float()  # Fix for dtype mismatch\n",
        "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
        "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
        "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
        "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
        "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Module 2 definition (DNN)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to process images\n",
        "def process_images(folder_path, transform, limit=None):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for label_dir in [\"0_real\", \"1_fake\"]:\n",
        "        label_path = os.path.join(folder_path, label_dir)\n",
        "        label = 0 if label_dir == \"0_real\" else 1\n",
        "\n",
        "        for i, fname in enumerate(tqdm(os.listdir(label_path), desc=f\"Processing {label_dir}\")):\n",
        "            if limit and len(images) >= limit:\n",
        "                break\n",
        "            img_path = os.path.join(label_path, fname)\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(Image.fromarray(image))\n",
        "                images.append(image)\n",
        "                filenames.append(fname)\n",
        "                labels.append(label)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X = torch.stack(images)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Process CSV features for Module 2\n",
        "def process_csv(path, limit=None):\n",
        "    df = pd.read_csv(path)\n",
        "    features = df['features'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
        "    filenames = df['image_name'].tolist()  # Ensure CSV has 'image_name' column\n",
        "    X = torch.tensor(features.tolist(), dtype=torch.float32)\n",
        "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "    if limit:\n",
        "        X = X[:limit]\n",
        "        filenames = filenames[:limit]\n",
        "        y = y[:limit]\n",
        "\n",
        "    return X, filenames, y\n",
        "\n",
        "\n",
        "# Set up Mediapipe for facial landmarks extraction\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "#import xception\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import os\n",
        "\n",
        "# Define the path where the model is saved\n",
        "drive_path = \"/content/drive/MyDrive/Final_folder_code_thesis/REALxception_model.pth\"\n",
        "\n",
        "# Ensure you are using the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Xception model without pretrained weights\n",
        "xception_model = pretrainedmodels.__dict__[\"xception\"](pretrained=None).to(device)\n",
        "\n",
        "# Modify the last linear layer to match the saved model\n",
        "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "if os.path.exists(drive_path):\n",
        "    print(\"Loading Xception model from Google Drive...\")\n",
        "    xception_model.load_state_dict(torch.load(drive_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found at {drive_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now the model is fully loaded with the correct architecture and weights\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
        "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
        "COCO_CLASSES = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def generate_sobel_edges(image, transform):\n",
        "    \"\"\"\n",
        "    Generates Sobel edges for a given image.\n",
        "    \"\"\"\n",
        "    gray_image = cv2.cvtColor(image.permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2GRAY)\n",
        "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
        "    sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
        "    sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
        "    return transform(Image.fromarray(sobel_combined))\n",
        "\n",
        "def extract_yolo_features_and_landmarks(image):\n",
        "    \"\"\"\n",
        "    Extracts YOLO object detection features and face landmarks from the given image.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image.permute(1, 2, 0).cpu().numpy())\n",
        "    detected_objects = []\n",
        "    landmarks = np.zeros((936,), dtype=np.float32)\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        class_id = int(result.cls[0])\n",
        "        class_name = yolo_model.names[class_id]\n",
        "\n",
        "        if class_name == \"person\":\n",
        "            crop = image.permute(1, 2, 0).cpu().numpy()[\n",
        "                int(result.xyxy[0][1]):int(result.xyxy[0][3]),\n",
        "                int(result.xyxy[0][0]):int(result.xyxy[0][2]),\n",
        "            ]\n",
        "            crop = (crop * 255).astype(np.uint8) if crop.max() <= 1.0 else crop.astype(np.uint8)\n",
        "            face_result = face_mesh.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            if face_result.multi_face_landmarks:\n",
        "                landmarks = np.array(\n",
        "                    [[p.x, p.y] for p in face_result.multi_face_landmarks[0].landmark]\n",
        "                ).flatten()\n",
        "\n",
        "        detected_objects.append(class_id)\n",
        "\n",
        "    yolo_features = torch.tensor([1 if i in detected_objects else 0 for i in range(len(COCO_CLASSES))])\n",
        "    return yolo_features, torch.tensor(landmarks)\n",
        "\n",
        "def prepare_ensemble_data(csv_path, folder_path, transform, module1, module2, batch_size=4):\n",
        "    \"\"\"\n",
        "    Prepares the ensemble dataset by processing data in batches.\n",
        "    \"\"\"\n",
        "    images, img_filenames, labels1 = process_images(folder_path, transform)\n",
        "    csv_features, csv_filenames, labels2 = process_csv(csv_path)\n",
        "\n",
        "    img_base_names = [os.path.splitext(fname)[0] for fname in img_filenames]\n",
        "    csv_base_names = [os.path.splitext(fname)[0] for fname in csv_filenames]\n",
        "\n",
        "    mapping = {f\"{base}_{labels1[i].item()}\": (i, None) for i, base in enumerate(img_base_names)}\n",
        "    for i, base in enumerate(csv_base_names):\n",
        "        key = f\"{base}_{labels2[i].item()}\"\n",
        "        if key in mapping:\n",
        "            mapping[key] = (mapping[key][0], i)\n",
        "\n",
        "    img_indices, csv_indices = [], []\n",
        "    for key, (img_idx, csv_idx) in mapping.items():\n",
        "        if csv_idx is not None:\n",
        "            img_indices.append(img_idx)\n",
        "            csv_indices.append(csv_idx)\n",
        "\n",
        "    images = images[img_indices]\n",
        "    labels1 = labels1[img_indices]\n",
        "    csv_features = csv_features[csv_indices]\n",
        "\n",
        "    combined_outputs_module1, combined_outputs_module2 = [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i + batch_size].to(device)\n",
        "        sobel_images, yolo_features, face_landmarks = [], [], []\n",
        "\n",
        "        for img in batch_images:\n",
        "            sobel_images.append(generate_sobel_edges(img, transform))\n",
        "            yolo, landmarks = extract_yolo_features_and_landmarks(img)\n",
        "            yolo_features.append(yolo)\n",
        "            face_landmarks.append(landmarks)\n",
        "\n",
        "        sobel_images = torch.stack(sobel_images).to(device)\n",
        "        yolo_features = torch.stack(yolo_features).to(device)\n",
        "        face_landmarks = torch.stack(face_landmarks).to(device)\n",
        "\n",
        "        # Get outputs from module1\n",
        "        module1.eval()\n",
        "        with torch.no_grad():\n",
        "            module1_output = module1(batch_images, sobel_images, yolo_features, face_landmarks)\n",
        "            combined_outputs_module1.append(module1_output.cpu())\n",
        "\n",
        "        # Get outputs from module2\n",
        "        module2.eval()\n",
        "        with torch.no_grad():\n",
        "            module2_output = module2(csv_features[i:i + batch_size].to(device))\n",
        "            combined_outputs_module2.append(module2_output.cpu())\n",
        "\n",
        "    module1_outputs = torch.cat(combined_outputs_module1, dim=0)\n",
        "    module2_outputs = torch.cat(combined_outputs_module2, dim=0)\n",
        "\n",
        "    # Ensure labels match the outputs\n",
        "    min_size = min(module1_outputs.size(0), labels1.size(0), module2_outputs.size(0))\n",
        "    module1_outputs = module1_outputs[:min_size]\n",
        "    module2_outputs = module2_outputs[:min_size]\n",
        "    labels1 = labels1[:min_size]\n",
        "\n",
        "    # Return the dataset\n",
        "    return TensorDataset(module1_outputs, module2_outputs, labels1)\n",
        "# Initialize models\n",
        "module1 = DeepfakeClassifier().to(device)\n",
        "module1.initialize_sobel_linear(input_shape=(3, 299, 299))\n",
        "module1.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/bedst_model_module2.pth\"))\n",
        "\n",
        "module2 = DNN(input_dim=768, hidden_dim_1=128, hidden_dim_2=256, output_dim=2).to(device)\n",
        "module2.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/final_model_1_weights.pth\"))\n",
        "\n",
        "# Freeze weights of module1 and module2\n",
        "for param in module1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in module2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "test_dataset = prepare_ensemble_data(test_csv, test_folder, transform, module1, module2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Ensemble model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, module1_dim, module2_dim, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(module1_dim + module2_dim, output_dim)  # Combine module1 and module2 outputs\n",
        "\n",
        "    def forward(self, x1_logits, x2_logits):\n",
        "        # Apply softmax to logits for probabilities\n",
        "        x1_probs = torch.softmax(x1_logits, dim=1)\n",
        "        x2_probs = torch.softmax(x2_logits, dim=1)\n",
        "        # Concatenate probabilities\n",
        "        combined_probs = torch.cat((x1_probs, x2_probs), dim=1)\n",
        "        # Pass through the fully connected layer\n",
        "        output = self.fc1(combined_probs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fCTrb-nlybC",
        "outputId": "69d4aae4-a156-4670-eba4-e9bb0857ce02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy twitter: 84.73%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the accuracy function\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for module1_out, module2_out, labels in dataloader:\n",
        "            module1_out, module2_out, labels = module1_out.to(device), module2_out.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(module1_out, module2_out)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Assuming EnsembleModel is defined and loaded correctly\n",
        "ensemble_model = EnsembleModel(module1_dim=2, module2_dim=2, output_dim=2).to(device)\n",
        "\n",
        "# Load the trained weights from the saved model\n",
        "ensemble_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Final_folder_code_thesis/Results_deepwild/ensemble_model_weights.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ensemble_model.eval()\n",
        "\n",
        "# Now compute the accuracy on the test set\n",
        "test_accuracy = compute_accuracy(ensemble_model, test_loader, device)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy twitter: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "ensemble_model.eval()\n",
        "with torch.no_grad():\n",
        "    for module1_out, module2_out, labels in test_loader:\n",
        "        module1_out, module2_out = module1_out.to(device), module2_out.to(device)\n",
        "        outputs = ensemble_model(module1_out, module2_out)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (fake)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# === ROC & AUC ===\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='magenta', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Deepfake Detection (Twitter Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=\"RdPu\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Deepfake Detection (Twitter Test Set)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# === Metrics ===\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, pos_label=1)\n",
        "recall = recall_score(all_labels, all_preds, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "print(f\"✅ Evaluation Results for Twitter Test Set\")\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"AUC Score      : {roc_auc:.4f}\")\n",
        "print(f\"Precision (Fake): {precision:.4f}\")\n",
        "print(f\"Recall (Fake)  : {recall:.4f}\")\n",
        "print(f\"F1 Score (Fake): {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "btoFS_pulj9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zTfuak0RElsw",
        "BmLhZkJCBEoM",
        "CwLnCJGDBOfR",
        "_drbJvJkKnve",
        "V7I6PTi1TDWT",
        "GQBmqPS7z5sI",
        "aUCPQs-7u2Y3",
        "q4rchIsU31fj",
        "akxV27LA7VzH"
      ],
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}